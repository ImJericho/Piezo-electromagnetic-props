{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1045, 15)\n",
      "Shape of y: (1045, 7)\n"
     ]
    }
   ],
   "source": [
    "# Load Data (replace this with actual CSV file or DataFrame)\n",
    "data = pd.read_csv('merged_properties.csv')  # Replace 'your_file.csv' with your actual file path\n",
    "# Features and targets\n",
    "# X = data.drop(columns=['c44', 'e15', 'q15', 'μ11', 'ϵ11', 'α11', 'ρ']).values\n",
    "X = data[['vf', 'c55e', 'e15e', 'q15e', 'ϵ11e', 'μ11e', 'α11e', 'ρe', 'c55f', 'e15f', 'q15f', 'ϵ11f', 'μ11f', 'α11f', 'ρf']].values\n",
    "y = data[['c44', 'e15', 'q15', 'μ11', 'ϵ11', 'α11', 'ρ']].values\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponents for X: [-1, 10, 1, 0, -7, -6, 0, 3, 11, 0, 2, -2, -3, -10, 3]\n",
      "Exponents for y: [10, 0, 1, -4, -2, -9, 3]\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty lists to store the exponents for X and y\n",
    "exponents_X = []\n",
    "exponents_y = []\n",
    "\n",
    "# Create copies of X and y to store the transformed data\n",
    "formal_X = X.copy()\n",
    "formal_y = y.copy()\n",
    "\n",
    "# Process X\n",
    "for i in range(X.shape[1]):\n",
    "    # Calculate the mean of the column\n",
    "    mean_value = X[:, i].mean()\n",
    "    \n",
    "    # Extract the exponent from the scientific notation of the mean value\n",
    "    exponent = int(f\"{mean_value:.1e}\".split('e')[1])\n",
    "    exponents_X.append(exponent)\n",
    "    \n",
    "    # Divide all values in the column by 10^exponent and round to 7 decimal places\n",
    "    formal_X[:, i] = (X[:, i] / (10 ** exponent)).round(7)\n",
    "\n",
    "# Process y\n",
    "for i in range(y.shape[1]):\n",
    "    # Calculate the mean of the column\n",
    "    mean_value = y[:, i].mean()\n",
    "    \n",
    "    # Extract the exponent from the scientific notation of the mean value\n",
    "    exponent = int(f\"{mean_value:.1e}\".split('e')[1])\n",
    "    exponents_y.append(exponent)\n",
    "    \n",
    "    # Divide all values in the column by 10^exponent and round to 7 decimal places\n",
    "    formal_y[:, i] = (y[:, i] / (10 ** exponent)).round(7)\n",
    "\n",
    "# Print the updated arrays and the lists of exponents\n",
    "print(\"Exponents for X:\", exponents_X)\n",
    "print(\"Exponents for y:\", exponents_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(formal_X)\n",
    "y_scaled = scaler_y.fit_transform(formal_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split first\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# scalers_X = []\n",
    "# X_train_scaled = np.zeros_like(X_train)\n",
    "# X_test_scaled = np.zeros_like(X_test)\n",
    "\n",
    "# for i in range(X.shape[1]):\n",
    "#     scaler = MinMaxScaler()\n",
    "#     X_train_scaled[:, i] = scaler.fit_transform(X_train[:, i].reshape(-1, 1)).flatten()\n",
    "#     X_test_scaled[:, i] = scaler.transform(X_test[:, i].reshape(-1, 1)).flatten()\n",
    "#     scalers_X.append(scaler)\n",
    "\n",
    "# # Per target scaling\n",
    "# scalers_y = []\n",
    "# y_train_scaled = np.zeros_like(y_train)\n",
    "# y_test_scaled = np.zeros_like(y_test)\n",
    "\n",
    "# for i in range(y_train.shape[1]):\n",
    "#     scaler = MinMaxScaler()\n",
    "#     y_train_scaled[:, i] = scaler.fit_transform(y_train[:, i].reshape(-1, 1)).flatten()\n",
    "#     y_test_scaled[:, i] = scaler.transform(y_test[:, i].reshape(-1, 1)).flatten()\n",
    "#     scalers_y.append(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Thesis/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 830us/step - loss: 1.0260 - mae: 0.3649 - val_loss: 0.6358 - val_mae: 0.1991\n",
      "Epoch 2/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.5786 - mae: 0.1790 - val_loss: 0.5348 - val_mae: 0.1936\n",
      "Epoch 3/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.4993 - mae: 0.1791 - val_loss: 0.4667 - val_mae: 0.1968\n",
      "Epoch 4/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.2207 - mae: 0.1258 - val_loss: 0.4892 - val_mae: 0.1405\n",
      "Epoch 5/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.9882 - mae: 0.1752 - val_loss: 0.4101 - val_mae: 0.1330\n",
      "Epoch 6/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.4082 - mae: 0.1255 - val_loss: 0.4075 - val_mae: 0.1172\n",
      "Epoch 7/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 0.4461 - mae: 0.1262 - val_loss: 0.4113 - val_mae: 0.1139\n",
      "Epoch 8/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - loss: 0.2974 - mae: 0.1047 - val_loss: 0.4312 - val_mae: 0.1287\n",
      "Epoch 9/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step - loss: 1.0018 - mae: 0.1612 - val_loss: 0.4394 - val_mae: 0.1248\n",
      "Epoch 10/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - loss: 0.4651 - mae: 0.1220 - val_loss: 0.6545 - val_mae: 0.1457\n",
      "Epoch 11/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - loss: 0.3431 - mae: 0.1051 - val_loss: 0.4805 - val_mae: 0.1277\n",
      "Epoch 12/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.3806 - mae: 0.1109 - val_loss: 0.5347 - val_mae: 0.1279\n",
      "Epoch 13/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.2199 - mae: 0.0862 - val_loss: 0.4002 - val_mae: 0.1189\n",
      "Epoch 14/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.1888 - mae: 0.0939 - val_loss: 0.4554 - val_mae: 0.1398\n",
      "Epoch 15/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - loss: 0.2988 - mae: 0.1056 - val_loss: 0.5614 - val_mae: 0.1304\n",
      "Epoch 16/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - loss: 0.6297 - mae: 0.1379 - val_loss: 0.3657 - val_mae: 0.1220\n",
      "Epoch 17/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - loss: 0.5040 - mae: 0.1264 - val_loss: 0.5603 - val_mae: 0.1217\n",
      "Epoch 18/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step - loss: 0.4288 - mae: 0.1123 - val_loss: 0.4997 - val_mae: 0.1166\n",
      "Epoch 19/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 0.4085 - mae: 0.1039 - val_loss: 0.4679 - val_mae: 0.1192\n",
      "Epoch 20/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.4261 - mae: 0.1142 - val_loss: 0.4583 - val_mae: 0.1108\n",
      "Epoch 21/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - loss: 0.5438 - mae: 0.1149 - val_loss: 0.4759 - val_mae: 0.1275\n",
      "Epoch 22/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.6940 - mae: 0.1363 - val_loss: 0.4963 - val_mae: 0.1136\n",
      "Epoch 23/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step - loss: 0.3471 - mae: 0.1077 - val_loss: 0.4098 - val_mae: 0.1104\n",
      "Epoch 24/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.2734 - mae: 0.0911 - val_loss: 0.5927 - val_mae: 0.1249\n",
      "Epoch 25/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.4060 - mae: 0.1211 - val_loss: 0.6436 - val_mae: 0.1278\n",
      "Epoch 26/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - loss: 0.4098 - mae: 0.1043 - val_loss: 0.4735 - val_mae: 0.1176\n",
      "Epoch 27/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.3560 - mae: 0.1017 - val_loss: 0.6321 - val_mae: 0.1318\n",
      "Epoch 28/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - loss: 0.2838 - mae: 0.0939 - val_loss: 0.4570 - val_mae: 0.1199\n",
      "Epoch 29/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - loss: 0.2231 - mae: 0.1051 - val_loss: 0.4242 - val_mae: 0.1092\n",
      "Epoch 30/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - loss: 0.5310 - mae: 0.1067 - val_loss: 0.3669 - val_mae: 0.1000\n",
      "Epoch 31/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.2789 - mae: 0.0950 - val_loss: 0.4577 - val_mae: 0.1127\n",
      "Epoch 32/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.5642 - mae: 0.1247 - val_loss: 0.5590 - val_mae: 0.1333\n",
      "Epoch 33/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - loss: 0.3216 - mae: 0.1035 - val_loss: 0.4609 - val_mae: 0.1178\n",
      "Epoch 34/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.2198 - mae: 0.0907 - val_loss: 0.4293 - val_mae: 0.1200\n",
      "Epoch 35/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.2801 - mae: 0.0952 - val_loss: 0.7059 - val_mae: 0.1355\n",
      "Epoch 36/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.2753 - mae: 0.1008 - val_loss: 0.6297 - val_mae: 0.1259\n",
      "Epoch 37/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - loss: 0.1482 - mae: 0.0860 - val_loss: 0.4265 - val_mae: 0.1232\n",
      "Epoch 38/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.1738 - mae: 0.0882 - val_loss: 0.3549 - val_mae: 0.1457\n",
      "Epoch 39/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.2199 - mae: 0.1007 - val_loss: 0.4658 - val_mae: 0.1132\n",
      "Epoch 40/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.2789 - mae: 0.1146 - val_loss: 0.4008 - val_mae: 0.1077\n",
      "Epoch 41/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.5522 - mae: 0.1243 - val_loss: 0.3963 - val_mae: 0.0996\n",
      "Epoch 42/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.2235 - mae: 0.0794 - val_loss: 0.4749 - val_mae: 0.1037\n",
      "Epoch 43/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.1404 - mae: 0.0699 - val_loss: 0.4321 - val_mae: 0.1144\n",
      "Epoch 44/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.5980 - mae: 0.1452 - val_loss: 0.5643 - val_mae: 0.1274\n",
      "Epoch 45/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.1485 - mae: 0.0791 - val_loss: 0.3900 - val_mae: 0.1162\n",
      "Epoch 46/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.4153 - mae: 0.1248 - val_loss: 0.4189 - val_mae: 0.0978\n",
      "Epoch 47/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - loss: 0.2631 - mae: 0.0899 - val_loss: 0.5869 - val_mae: 0.1168\n",
      "Epoch 48/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - loss: 0.2346 - mae: 0.0931 - val_loss: 0.4392 - val_mae: 0.1117\n",
      "Epoch 49/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - loss: 0.2184 - mae: 0.0908 - val_loss: 0.5086 - val_mae: 0.0990\n",
      "Epoch 50/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 0.2363 - mae: 0.0811 - val_loss: 0.4258 - val_mae: 0.0951\n",
      "Epoch 51/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - loss: 0.1330 - mae: 0.0660 - val_loss: 0.5493 - val_mae: 0.1113\n",
      "Epoch 52/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - loss: 0.1644 - mae: 0.0779 - val_loss: 0.3911 - val_mae: 0.1000\n",
      "Epoch 53/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.1835 - mae: 0.0810 - val_loss: 0.3568 - val_mae: 0.1097\n",
      "Epoch 54/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.3265 - mae: 0.1073 - val_loss: 0.3670 - val_mae: 0.1011\n",
      "Epoch 55/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.2543 - mae: 0.0814 - val_loss: 0.4955 - val_mae: 0.1031\n",
      "Epoch 56/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - loss: 0.1669 - mae: 0.0699 - val_loss: 0.4461 - val_mae: 0.1046\n",
      "Epoch 57/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step - loss: 0.3842 - mae: 0.1027 - val_loss: 0.3741 - val_mae: 0.1099\n",
      "Epoch 58/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 0.1689 - mae: 0.0832 - val_loss: 0.3120 - val_mae: 0.1074\n",
      "Epoch 59/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 0.1809 - mae: 0.0803 - val_loss: 0.4307 - val_mae: 0.1027\n",
      "Epoch 60/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.6269 - mae: 0.1154 - val_loss: 0.3376 - val_mae: 0.0832\n",
      "Epoch 61/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 0.1475 - mae: 0.0633 - val_loss: 0.3955 - val_mae: 0.1026\n",
      "Epoch 62/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.1601 - mae: 0.0669 - val_loss: 0.3738 - val_mae: 0.0896\n",
      "Epoch 63/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.1412 - mae: 0.0674 - val_loss: 0.3703 - val_mae: 0.0929\n",
      "Epoch 64/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 0.2324 - mae: 0.0861 - val_loss: 0.5634 - val_mae: 0.1094\n",
      "Epoch 65/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.1991 - mae: 0.0856 - val_loss: 0.5351 - val_mae: 0.1079\n",
      "Epoch 66/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.1894 - mae: 0.0749 - val_loss: 0.3366 - val_mae: 0.1005\n",
      "Epoch 67/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.2712 - mae: 0.0921 - val_loss: 0.3539 - val_mae: 0.0860\n",
      "Epoch 68/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.4927 - mae: 0.1085 - val_loss: 0.4920 - val_mae: 0.1051\n",
      "Epoch 69/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.4307 - mae: 0.1059 - val_loss: 0.4319 - val_mae: 0.0891\n",
      "Epoch 70/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.1977 - mae: 0.0737 - val_loss: 0.4249 - val_mae: 0.0961\n",
      "Epoch 71/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.1957 - mae: 0.0676 - val_loss: 0.3895 - val_mae: 0.0961\n",
      "Epoch 72/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.2260 - mae: 0.0798 - val_loss: 0.3859 - val_mae: 0.0996\n",
      "Epoch 73/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.3463 - mae: 0.0943 - val_loss: 0.4029 - val_mae: 0.1018\n",
      "Epoch 74/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step - loss: 0.1181 - mae: 0.0586 - val_loss: 0.4941 - val_mae: 0.1070\n",
      "Epoch 75/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.1022 - mae: 0.0546 - val_loss: 0.4842 - val_mae: 0.1223\n",
      "Epoch 76/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.1143 - mae: 0.0655 - val_loss: 0.4942 - val_mae: 0.1178\n",
      "Epoch 77/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.1782 - mae: 0.0799 - val_loss: 0.4274 - val_mae: 0.1043\n",
      "Epoch 78/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.2780 - mae: 0.0905 - val_loss: 0.4175 - val_mae: 0.0900\n",
      "Epoch 79/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.0544 - mae: 0.0467 - val_loss: 0.4693 - val_mae: 0.1101\n",
      "Epoch 80/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.2481 - mae: 0.0935 - val_loss: 0.4289 - val_mae: 0.0990\n",
      "Epoch 81/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.1041 - mae: 0.0605 - val_loss: 0.4851 - val_mae: 0.1008\n",
      "Epoch 82/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.1569 - mae: 0.0642 - val_loss: 0.4205 - val_mae: 0.1212\n",
      "Epoch 83/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.4646 - mae: 0.1151 - val_loss: 0.3598 - val_mae: 0.1065\n",
      "Epoch 84/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.1244 - mae: 0.0725 - val_loss: 0.4745 - val_mae: 0.0980\n",
      "Epoch 85/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.2236 - mae: 0.0940 - val_loss: 0.4152 - val_mae: 0.0982\n",
      "Epoch 86/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.0949 - mae: 0.0603 - val_loss: 0.5352 - val_mae: 0.0945\n",
      "Epoch 87/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - loss: 0.2552 - mae: 0.0788 - val_loss: 0.3828 - val_mae: 0.1069\n",
      "Epoch 88/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.4312 - mae: 0.1033 - val_loss: 0.3346 - val_mae: 0.0844\n",
      "Epoch 89/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.3199 - mae: 0.0778 - val_loss: 0.3569 - val_mae: 0.0974\n",
      "Epoch 90/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.2227 - mae: 0.0774 - val_loss: 0.3615 - val_mae: 0.0883\n",
      "Epoch 91/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.1891 - mae: 0.0833 - val_loss: 0.5688 - val_mae: 0.1027\n",
      "Epoch 92/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.4601 - mae: 0.0886 - val_loss: 0.4890 - val_mae: 0.1052\n",
      "Epoch 93/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.1954 - mae: 0.0721 - val_loss: 0.3763 - val_mae: 0.0961\n",
      "Epoch 94/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.3605 - mae: 0.0954 - val_loss: 0.4219 - val_mae: 0.0897\n",
      "Epoch 95/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.2224 - mae: 0.0767 - val_loss: 0.3532 - val_mae: 0.0904\n",
      "Epoch 96/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0727 - mae: 0.0549 - val_loss: 0.4582 - val_mae: 0.0973\n",
      "Epoch 97/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.1992 - mae: 0.0827 - val_loss: 0.3257 - val_mae: 0.0946\n",
      "Epoch 98/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.2042 - mae: 0.0799 - val_loss: 0.3491 - val_mae: 0.0851\n",
      "Epoch 99/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.0676 - mae: 0.0512 - val_loss: 0.5043 - val_mae: 0.1124\n",
      "Epoch 100/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.1059 - mae: 0.0674 - val_loss: 0.3783 - val_mae: 0.0936\n",
      "Epoch 101/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - loss: 0.2807 - mae: 0.0794 - val_loss: 0.3096 - val_mae: 0.0959\n",
      "Epoch 102/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 0.2843 - mae: 0.0911 - val_loss: 0.4074 - val_mae: 0.0897\n",
      "Epoch 103/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.1406 - mae: 0.0630 - val_loss: 0.4407 - val_mae: 0.1031\n",
      "Epoch 104/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.3238 - mae: 0.0883 - val_loss: 0.4209 - val_mae: 0.0885\n",
      "Epoch 105/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.1612 - mae: 0.0662 - val_loss: 0.4005 - val_mae: 0.0934\n",
      "Epoch 106/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.1324 - mae: 0.0644 - val_loss: 0.3813 - val_mae: 0.0950\n",
      "Epoch 107/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.1454 - mae: 0.0663 - val_loss: 0.3848 - val_mae: 0.0898\n",
      "Epoch 108/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.1020 - mae: 0.0507 - val_loss: 0.3388 - val_mae: 0.0960\n",
      "Epoch 109/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.1045 - mae: 0.0602 - val_loss: 0.4534 - val_mae: 0.1021\n",
      "Epoch 110/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.4254 - mae: 0.1018 - val_loss: 0.3716 - val_mae: 0.0905\n",
      "Epoch 111/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.1348 - mae: 0.0597 - val_loss: 0.3953 - val_mae: 0.0992\n",
      "Epoch 112/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 0.0791 - mae: 0.0561 - val_loss: 0.3770 - val_mae: 0.0935\n",
      "Epoch 113/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.0886 - mae: 0.0576 - val_loss: 0.5213 - val_mae: 0.0967\n",
      "Epoch 114/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.3228 - mae: 0.0897 - val_loss: 0.4157 - val_mae: 0.0956\n",
      "Epoch 115/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.1348 - mae: 0.0719 - val_loss: 0.3856 - val_mae: 0.0881\n",
      "Epoch 116/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.1210 - mae: 0.0641 - val_loss: 0.3868 - val_mae: 0.0921\n",
      "Epoch 117/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.1185 - mae: 0.0619 - val_loss: 0.2876 - val_mae: 0.0917\n",
      "Epoch 118/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.1189 - mae: 0.0655 - val_loss: 0.3343 - val_mae: 0.0828\n",
      "Epoch 119/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.0594 - mae: 0.0452 - val_loss: 0.3308 - val_mae: 0.0851\n",
      "Epoch 120/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.0717 - mae: 0.0548 - val_loss: 0.3763 - val_mae: 0.0881\n",
      "Epoch 121/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0940 - mae: 0.0634 - val_loss: 0.4750 - val_mae: 0.0966\n",
      "Epoch 122/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.1733 - mae: 0.0923 - val_loss: 0.4516 - val_mae: 0.0987\n",
      "Epoch 123/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.0694 - mae: 0.0500 - val_loss: 0.5291 - val_mae: 0.1255\n",
      "Epoch 124/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.1467 - mae: 0.0862 - val_loss: 0.3738 - val_mae: 0.1009\n",
      "Epoch 125/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 0.0878 - mae: 0.0646 - val_loss: 0.5754 - val_mae: 0.1072\n",
      "Epoch 126/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0845 - mae: 0.0532 - val_loss: 0.5238 - val_mae: 0.1053\n",
      "Epoch 127/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - loss: 0.1062 - mae: 0.0553 - val_loss: 0.3969 - val_mae: 0.1011\n",
      "Epoch 128/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.1676 - mae: 0.0566 - val_loss: 0.4219 - val_mae: 0.0912\n",
      "Epoch 129/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.0418 - mae: 0.0451 - val_loss: 0.3802 - val_mae: 0.0983\n",
      "Epoch 130/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 0.0895 - mae: 0.0582 - val_loss: 0.5015 - val_mae: 0.0973\n",
      "Epoch 131/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.0873 - mae: 0.0563 - val_loss: 0.4664 - val_mae: 0.0955\n",
      "Epoch 132/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.0282 - mae: 0.0384 - val_loss: 0.3624 - val_mae: 0.0843\n",
      "Epoch 133/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.0414 - mae: 0.0492 - val_loss: 0.3866 - val_mae: 0.0901\n",
      "Epoch 134/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.1267 - mae: 0.0526 - val_loss: 0.4312 - val_mae: 0.0977\n",
      "Epoch 135/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.1179 - mae: 0.0603 - val_loss: 0.4222 - val_mae: 0.0971\n",
      "Epoch 136/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.1157 - mae: 0.0599 - val_loss: 0.4950 - val_mae: 0.1048\n",
      "Epoch 137/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 0.1975 - mae: 0.0748 - val_loss: 0.4346 - val_mae: 0.0897\n",
      "Epoch 138/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step - loss: 0.2015 - mae: 0.0625 - val_loss: 0.4730 - val_mae: 0.1162\n",
      "Epoch 139/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0289 - mae: 0.0483 - val_loss: 0.4581 - val_mae: 0.0928\n",
      "Epoch 140/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - loss: 0.0348 - mae: 0.0460 - val_loss: 0.4152 - val_mae: 0.0900\n",
      "Epoch 141/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.0280 - mae: 0.0379 - val_loss: 0.3693 - val_mae: 0.0884\n",
      "Epoch 142/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.0668 - mae: 0.0493 - val_loss: 0.3589 - val_mae: 0.0889\n",
      "Epoch 143/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.0660 - mae: 0.0490 - val_loss: 0.3509 - val_mae: 0.0946\n",
      "Epoch 144/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.0321 - mae: 0.0467 - val_loss: 0.3178 - val_mae: 0.0794\n",
      "Epoch 145/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.0236 - mae: 0.0417 - val_loss: 0.4505 - val_mae: 0.0879\n",
      "Epoch 146/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.1410 - mae: 0.0571 - val_loss: 0.3504 - val_mae: 0.0974\n",
      "Epoch 147/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.5499 - mae: 0.1033 - val_loss: 0.4315 - val_mae: 0.1020\n",
      "Epoch 148/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.1459 - mae: 0.0639 - val_loss: 0.3705 - val_mae: 0.0856\n",
      "Epoch 149/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.0763 - mae: 0.0498 - val_loss: 0.3443 - val_mae: 0.0833\n",
      "Epoch 150/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.1856 - mae: 0.0610 - val_loss: 0.5788 - val_mae: 0.1010\n",
      "Epoch 151/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - loss: 0.2587 - mae: 0.0713 - val_loss: 0.2898 - val_mae: 0.0884\n",
      "Epoch 152/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517us/step - loss: 0.3443 - mae: 0.0701 - val_loss: 0.3964 - val_mae: 0.0917\n",
      "Epoch 153/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.0362 - mae: 0.0430 - val_loss: 0.3697 - val_mae: 0.0885\n",
      "Epoch 154/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0377 - mae: 0.0459 - val_loss: 0.4952 - val_mae: 0.0971\n",
      "Epoch 155/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.0764 - mae: 0.0501 - val_loss: 0.3527 - val_mae: 0.0902\n",
      "Epoch 156/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0948 - mae: 0.0570 - val_loss: 0.4266 - val_mae: 0.0902\n",
      "Epoch 157/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0940 - mae: 0.0491 - val_loss: 0.2675 - val_mae: 0.0745\n",
      "Epoch 158/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.0169 - mae: 0.0332 - val_loss: 0.3849 - val_mae: 0.0845\n",
      "Epoch 159/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0473 - mae: 0.0391 - val_loss: 0.4824 - val_mae: 0.0976\n",
      "Epoch 160/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.0354 - mae: 0.0473 - val_loss: 0.3142 - val_mae: 0.0891\n",
      "Epoch 161/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.1374 - mae: 0.0508 - val_loss: 0.4039 - val_mae: 0.0892\n",
      "Epoch 162/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.0213 - mae: 0.0388 - val_loss: 0.3542 - val_mae: 0.0804\n",
      "Epoch 163/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.0244 - mae: 0.0339 - val_loss: 0.3130 - val_mae: 0.0735\n",
      "Epoch 164/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.0128 - mae: 0.0297 - val_loss: 0.4315 - val_mae: 0.0906\n",
      "Epoch 165/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.0408 - mae: 0.0430 - val_loss: 0.4497 - val_mae: 0.0977\n",
      "Epoch 166/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.0999 - mae: 0.0496 - val_loss: 0.3786 - val_mae: 0.1048\n",
      "Epoch 167/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 0.1582 - mae: 0.0691 - val_loss: 0.3277 - val_mae: 0.1034\n",
      "Epoch 168/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.1268 - mae: 0.0725 - val_loss: 0.4485 - val_mae: 0.0961\n",
      "Epoch 169/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.2151 - mae: 0.0631 - val_loss: 0.4090 - val_mae: 0.0955\n",
      "Epoch 170/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.0975 - mae: 0.0536 - val_loss: 0.4147 - val_mae: 0.0944\n",
      "Epoch 171/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0381 - mae: 0.0427 - val_loss: 0.5413 - val_mae: 0.1101\n",
      "Epoch 172/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0943 - mae: 0.0613 - val_loss: 0.5087 - val_mae: 0.1099\n",
      "Epoch 173/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.4310 - mae: 0.0818 - val_loss: 0.4878 - val_mae: 0.1012\n",
      "Epoch 174/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.0284 - mae: 0.0387 - val_loss: 0.3946 - val_mae: 0.0887\n",
      "Epoch 175/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.0220 - mae: 0.0369 - val_loss: 0.5203 - val_mae: 0.0916\n",
      "Epoch 176/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.0226 - mae: 0.0327 - val_loss: 0.4836 - val_mae: 0.0917\n",
      "Epoch 177/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.0176 - mae: 0.0307 - val_loss: 0.3767 - val_mae: 0.0900\n",
      "Epoch 178/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - loss: 0.1572 - mae: 0.0570 - val_loss: 0.3803 - val_mae: 0.0865\n",
      "Epoch 179/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.0269 - mae: 0.0408 - val_loss: 0.3314 - val_mae: 0.0783\n",
      "Epoch 180/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.0501 - mae: 0.0445 - val_loss: 0.3613 - val_mae: 0.0812\n",
      "Epoch 181/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.0177 - mae: 0.0344 - val_loss: 0.4658 - val_mae: 0.0881\n",
      "Epoch 182/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0095 - mae: 0.0290 - val_loss: 0.3379 - val_mae: 0.0787\n",
      "Epoch 183/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0049 - mae: 0.0271 - val_loss: 0.4243 - val_mae: 0.0855\n",
      "Epoch 184/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0155 - mae: 0.0317 - val_loss: 0.4781 - val_mae: 0.0959\n",
      "Epoch 185/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.0903 - mae: 0.0571 - val_loss: 0.4587 - val_mae: 0.1156\n",
      "Epoch 186/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 0.6756 - mae: 0.1137 - val_loss: 0.3618 - val_mae: 0.0903\n",
      "Epoch 187/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.1472 - mae: 0.0626 - val_loss: 0.4622 - val_mae: 0.1036\n",
      "Epoch 188/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.1440 - mae: 0.0632 - val_loss: 0.5326 - val_mae: 0.1103\n",
      "Epoch 189/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.1144 - mae: 0.0666 - val_loss: 0.5286 - val_mae: 0.0942\n",
      "Epoch 190/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - loss: 0.2443 - mae: 0.0668 - val_loss: 0.5692 - val_mae: 0.1102\n",
      "Epoch 191/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.2227 - mae: 0.0694 - val_loss: 0.4452 - val_mae: 0.0873\n",
      "Epoch 192/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0256 - mae: 0.0349 - val_loss: 0.4720 - val_mae: 0.0890\n",
      "Epoch 193/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.0167 - mae: 0.0315 - val_loss: 0.3145 - val_mae: 0.0780\n",
      "Epoch 194/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.0157 - mae: 0.0345 - val_loss: 0.4642 - val_mae: 0.0908\n",
      "Epoch 195/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.0148 - mae: 0.0307 - val_loss: 0.6568 - val_mae: 0.1082\n",
      "Epoch 196/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.2311 - mae: 0.0659 - val_loss: 0.3102 - val_mae: 0.0841\n",
      "Epoch 197/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - loss: 0.0739 - mae: 0.0491 - val_loss: 0.4053 - val_mae: 0.0862\n",
      "Epoch 198/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.1520 - mae: 0.0503 - val_loss: 0.3551 - val_mae: 0.0831\n",
      "Epoch 199/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.0155 - mae: 0.0332 - val_loss: 0.3644 - val_mae: 0.0803\n",
      "Epoch 200/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.0094 - mae: 0.0267 - val_loss: 0.4457 - val_mae: 0.0844\n",
      "Epoch 201/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.0038 - mae: 0.0243 - val_loss: 0.3393 - val_mae: 0.0759\n",
      "Epoch 202/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.0061 - mae: 0.0261 - val_loss: 0.4432 - val_mae: 0.0849\n",
      "Epoch 203/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - loss: 0.0157 - mae: 0.0307 - val_loss: 0.4482 - val_mae: 0.0899\n",
      "Epoch 204/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.0055 - mae: 0.0268 - val_loss: 0.4002 - val_mae: 0.0812\n",
      "Epoch 205/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0038 - mae: 0.0240 - val_loss: 0.3313 - val_mae: 0.0765\n",
      "Epoch 206/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.0027 - mae: 0.0228 - val_loss: 0.4260 - val_mae: 0.0836\n",
      "Epoch 207/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.0034 - mae: 0.0244 - val_loss: 0.3340 - val_mae: 0.0801\n",
      "Epoch 208/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 0.0435 - mae: 0.0416 - val_loss: 0.3308 - val_mae: 0.0853\n",
      "Epoch 209/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.3565 - mae: 0.0825 - val_loss: 0.4055 - val_mae: 0.1070\n",
      "Epoch 210/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.2521 - mae: 0.0826 - val_loss: 0.4324 - val_mae: 0.1181\n",
      "Epoch 211/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.4764 - mae: 0.1111 - val_loss: 0.5294 - val_mae: 0.1233\n",
      "Epoch 212/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.1653 - mae: 0.0950 - val_loss: 0.4095 - val_mae: 0.1025\n",
      "Epoch 213/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.1649 - mae: 0.0744 - val_loss: 0.5215 - val_mae: 0.1046\n",
      "Epoch 214/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.0857 - mae: 0.0558 - val_loss: 0.4751 - val_mae: 0.0987\n",
      "Epoch 215/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.0577 - mae: 0.0537 - val_loss: 0.5104 - val_mae: 0.0975\n",
      "Epoch 216/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - loss: 0.0769 - mae: 0.0533 - val_loss: 0.3807 - val_mae: 0.0911\n",
      "Epoch 217/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.0641 - mae: 0.0485 - val_loss: 0.3671 - val_mae: 0.0962\n",
      "Epoch 218/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.1717 - mae: 0.0642 - val_loss: 0.5316 - val_mae: 0.0998\n",
      "Epoch 219/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.0189 - mae: 0.0367 - val_loss: 0.5519 - val_mae: 0.0968\n",
      "Epoch 220/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.0065 - mae: 0.0288 - val_loss: 0.3950 - val_mae: 0.0891\n",
      "Epoch 221/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.0206 - mae: 0.0361 - val_loss: 0.3651 - val_mae: 0.0847\n",
      "Epoch 222/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.0054 - mae: 0.0260 - val_loss: 0.3414 - val_mae: 0.0819\n",
      "Epoch 223/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0242 - mae: 0.0333 - val_loss: 0.4058 - val_mae: 0.0985\n",
      "Epoch 224/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.4780 - mae: 0.0887 - val_loss: 0.4080 - val_mae: 0.0926\n",
      "Epoch 225/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.1185 - mae: 0.0533 - val_loss: 0.4681 - val_mae: 0.1025\n",
      "Epoch 226/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.0915 - mae: 0.0615 - val_loss: 0.4577 - val_mae: 0.0930\n",
      "Epoch 227/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.1567 - mae: 0.0638 - val_loss: 0.4147 - val_mae: 0.0868\n",
      "Epoch 228/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - loss: 0.1094 - mae: 0.0585 - val_loss: 0.4660 - val_mae: 0.0922\n",
      "Epoch 229/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.4795 - mae: 0.0740 - val_loss: 0.3767 - val_mae: 0.0882\n",
      "Epoch 230/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.0240 - mae: 0.0390 - val_loss: 0.4317 - val_mae: 0.0869\n",
      "Epoch 231/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.0217 - mae: 0.0331 - val_loss: 0.4610 - val_mae: 0.0868\n",
      "Epoch 232/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 0.0315 - mae: 0.0357 - val_loss: 0.3344 - val_mae: 0.0752\n",
      "Epoch 233/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0572 - mae: 0.0410 - val_loss: 0.3718 - val_mae: 0.0800\n",
      "Epoch 234/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.3324 - mae: 0.0727 - val_loss: 0.3880 - val_mae: 0.0881\n",
      "Epoch 235/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - loss: 0.0284 - mae: 0.0363 - val_loss: 0.3046 - val_mae: 0.0777\n",
      "Epoch 236/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.0158 - mae: 0.0305 - val_loss: 0.4934 - val_mae: 0.0910\n",
      "Epoch 237/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.0151 - mae: 0.0335 - val_loss: 0.4116 - val_mae: 0.0853\n",
      "Epoch 238/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.0086 - mae: 0.0280 - val_loss: 0.3446 - val_mae: 0.0791\n",
      "Epoch 239/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.0116 - mae: 0.0299 - val_loss: 0.4283 - val_mae: 0.0870\n",
      "Epoch 240/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.0103 - mae: 0.0283 - val_loss: 0.4621 - val_mae: 0.0940\n",
      "Epoch 241/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - loss: 0.0068 - mae: 0.0268 - val_loss: 0.3279 - val_mae: 0.0780\n",
      "Epoch 242/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.0119 - mae: 0.0307 - val_loss: 0.4667 - val_mae: 0.0870\n",
      "Epoch 243/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.0186 - mae: 0.0324 - val_loss: 0.3690 - val_mae: 0.0816\n",
      "Epoch 244/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0161 - mae: 0.0318 - val_loss: 0.3720 - val_mae: 0.0825\n",
      "Epoch 245/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.2581 - mae: 0.0734 - val_loss: 0.3761 - val_mae: 0.1023\n",
      "Epoch 246/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.2669 - mae: 0.0859 - val_loss: 0.5658 - val_mae: 0.1124\n",
      "Epoch 247/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - loss: 0.1241 - mae: 0.0682 - val_loss: 0.5030 - val_mae: 0.1089\n",
      "Epoch 248/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 0.2336 - mae: 0.0824 - val_loss: 0.4393 - val_mae: 0.0964\n",
      "Epoch 249/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.2426 - mae: 0.0749 - val_loss: 0.5359 - val_mae: 0.1094\n",
      "Epoch 250/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step - loss: 0.1782 - mae: 0.0718 - val_loss: 0.5435 - val_mae: 0.1126\n",
      "Epoch 251/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - loss: 0.1225 - mae: 0.0653 - val_loss: 0.3804 - val_mae: 0.0874\n",
      "Epoch 252/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - loss: 0.3572 - mae: 0.0923 - val_loss: 0.4677 - val_mae: 0.1080\n",
      "Epoch 253/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 0.1928 - mae: 0.0739 - val_loss: 0.3952 - val_mae: 0.0933\n",
      "Epoch 254/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - loss: 0.2650 - mae: 0.0741 - val_loss: 0.4946 - val_mae: 0.1029\n",
      "Epoch 255/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.2613 - mae: 0.0731 - val_loss: 0.4775 - val_mae: 0.1021\n",
      "Epoch 256/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - loss: 0.2439 - mae: 0.0754 - val_loss: 0.4146 - val_mae: 0.0891\n",
      "Epoch 257/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.1617 - mae: 0.0584 - val_loss: 0.3388 - val_mae: 0.0936\n",
      "Epoch 258/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.0767 - mae: 0.0547 - val_loss: 0.4402 - val_mae: 0.0964\n",
      "Epoch 259/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - loss: 0.2021 - mae: 0.0661 - val_loss: 0.3786 - val_mae: 0.0899\n",
      "Epoch 260/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.1105 - mae: 0.0599 - val_loss: 0.4367 - val_mae: 0.0971\n",
      "Epoch 261/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.1420 - mae: 0.0605 - val_loss: 0.3287 - val_mae: 0.0860\n",
      "Epoch 262/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 0.1282 - mae: 0.0569 - val_loss: 0.5638 - val_mae: 0.1069\n",
      "Epoch 263/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 0.3330 - mae: 0.0845 - val_loss: 0.4895 - val_mae: 0.1123\n",
      "Epoch 264/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.1484 - mae: 0.0637 - val_loss: 0.5013 - val_mae: 0.1047\n",
      "Epoch 265/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - loss: 0.1251 - mae: 0.0596 - val_loss: 0.5437 - val_mae: 0.1082\n",
      "Epoch 266/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 0.1945 - mae: 0.0740 - val_loss: 0.4081 - val_mae: 0.0959\n",
      "Epoch 267/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.2452 - mae: 0.0876 - val_loss: 0.4997 - val_mae: 0.1093\n",
      "Epoch 268/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - loss: 0.1849 - mae: 0.0672 - val_loss: 0.7865 - val_mae: 0.1263\n",
      "Epoch 269/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - loss: 0.4968 - mae: 0.0984 - val_loss: 0.3789 - val_mae: 0.0853\n",
      "Epoch 270/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.1476 - mae: 0.0653 - val_loss: 0.4968 - val_mae: 0.0958\n",
      "Epoch 271/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 0.1908 - mae: 0.0715 - val_loss: 0.4635 - val_mae: 0.0985\n",
      "Epoch 272/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.1120 - mae: 0.0520 - val_loss: 0.4936 - val_mae: 0.0916\n",
      "Epoch 273/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.1278 - mae: 0.0630 - val_loss: 0.4051 - val_mae: 0.0926\n",
      "Epoch 274/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0950 - mae: 0.0564 - val_loss: 0.4273 - val_mae: 0.0893\n",
      "Epoch 275/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.1016 - mae: 0.0629 - val_loss: 0.4534 - val_mae: 0.0983\n",
      "Epoch 276/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0753 - mae: 0.0511 - val_loss: 0.5026 - val_mae: 0.0980\n",
      "Epoch 277/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.1962 - mae: 0.0789 - val_loss: 0.3920 - val_mae: 0.0910\n",
      "Epoch 278/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - loss: 0.1583 - mae: 0.0582 - val_loss: 0.4407 - val_mae: 0.0958\n",
      "Epoch 279/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.0846 - mae: 0.0557 - val_loss: 0.3627 - val_mae: 0.0877\n",
      "Epoch 280/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.0923 - mae: 0.0578 - val_loss: 0.4777 - val_mae: 0.0955\n",
      "Epoch 281/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.0333 - mae: 0.0435 - val_loss: 0.3777 - val_mae: 0.0948\n",
      "Epoch 282/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.1000 - mae: 0.0589 - val_loss: 0.4255 - val_mae: 0.1019\n",
      "Epoch 283/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.3908 - mae: 0.0930 - val_loss: 0.4298 - val_mae: 0.0891\n",
      "Epoch 284/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0968 - mae: 0.0513 - val_loss: 0.3934 - val_mae: 0.0901\n",
      "Epoch 285/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0894 - mae: 0.0602 - val_loss: 0.3728 - val_mae: 0.0899\n",
      "Epoch 286/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.0603 - mae: 0.0568 - val_loss: 0.4631 - val_mae: 0.0917\n",
      "Epoch 287/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.0555 - mae: 0.0506 - val_loss: 0.3674 - val_mae: 0.1013\n",
      "Epoch 288/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.2543 - mae: 0.0729 - val_loss: 0.4123 - val_mae: 0.0903\n",
      "Epoch 289/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.0547 - mae: 0.0440 - val_loss: 0.3064 - val_mae: 0.0928\n",
      "Epoch 290/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - loss: 0.0624 - mae: 0.0496 - val_loss: 0.4423 - val_mae: 0.0941\n",
      "Epoch 291/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - loss: 0.0191 - mae: 0.0360 - val_loss: 0.3620 - val_mae: 0.0902\n",
      "Epoch 292/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 0.0327 - mae: 0.0439 - val_loss: 0.5018 - val_mae: 0.1004\n",
      "Epoch 293/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 0.0397 - mae: 0.0511 - val_loss: 0.7098 - val_mae: 0.1199\n",
      "Epoch 294/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 0.1067 - mae: 0.0713 - val_loss: 0.3832 - val_mae: 0.0956\n",
      "Epoch 295/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - loss: 0.1196 - mae: 0.0625 - val_loss: 0.4525 - val_mae: 0.1100\n",
      "Epoch 296/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.1709 - mae: 0.0717 - val_loss: 0.4698 - val_mae: 0.0982\n",
      "Epoch 297/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.0396 - mae: 0.0517 - val_loss: 0.3794 - val_mae: 0.0847\n",
      "Epoch 298/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - loss: 0.0208 - mae: 0.0355 - val_loss: 0.3638 - val_mae: 0.0820\n",
      "Epoch 299/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.0082 - mae: 0.0331 - val_loss: 0.4530 - val_mae: 0.0945\n",
      "Epoch 300/300\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.0120 - mae: 0.0381 - val_loss: 0.3613 - val_mae: 0.0837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x14b185ee0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(15,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(7, activation='linear')  # 7 target outputs\n",
    "])\n",
    "\n",
    "# Compile with standard MSE\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Training\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=300,\n",
    "    batch_size=4,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.0837\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Predicted: [ 3.8434527  -1.1806159  11.235526    0.238786   -0.15530989 -0.17421177  7.329519  ]\n",
      "Actual:    [ 5.75659230e+00  3.79153000e-02  1.01572435e+01  3.00662100e-01  0.00000000e+00 -6.17500000e-04  7.33000000e+00]\n",
      "--------------------------------------------------\n",
      "Predicted: [ 8.9802475e+00  1.9519508e+00 -7.7989376e-01  7.2907939e+00 -4.5597181e-03  1.3536985e+00  7.0633774e+00]\n",
      "Actual:    [9.6557162e+00 5.6567740e-01 1.4630000e-04 7.1873362e+00 1.0000000e-07 2.8740900e-01 7.1000000e+00]\n",
      "--------------------------------------------------\n",
      "Predicted: [ 4.5754986   6.360233    0.04457579 32.081406    0.03919689 -0.12595204  5.295139  ]\n",
      "Actual:    [3.06869110e+00 3.47900000e-04 9.99999998e-08 3.18686247e+01 0.00000000e+00 7.08191700e-01 5.28000000e+00]\n",
      "--------------------------------------------------\n",
      "Predicted: [ 26.207727   125.388176    23.664383     0.21520497  13.834323   -45.986965     8.082935  ]\n",
      "Actual:    [ 12.3707914  69.7810465  17.9492512   0.112323   10.0493513 -39.3328375   8.1575   ]\n",
      "--------------------------------------------------\n",
      "Predicted: [ 4.2233024   5.8027153  -0.08973254 35.087555    0.28551796  0.17308924  5.4179707 ]\n",
      "Actual:    [3.35672220e+00 6.05600000e-04 2.00000000e-07 3.50214417e+01 0.00000000e+00 7.78254200e-01 5.43000000e+00]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse scaling for y_pred and y_test\n",
    "y_pred_original = scaler_y.inverse_transform(y_pred)\n",
    "y_test_original = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "np.set_printoptions(linewidth=np.inf)  # Set the print options to avoid line breaks\n",
    "for i in range(5):\n",
    "    print(f\"Predicted: {y_pred_original[i]}\")\n",
    "    print(f\"Actual:    {y_test_original[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 0, 1, -4, -2, -9, 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exponents_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Properties:\n",
      "c55: 91987156992.0000\n",
      "e15: -4.3636\n",
      "q15: 430.8117\n",
      "ϵ11: 0.0001\n",
      "μ11: 0.0043\n",
      "α11: 0.0000\n",
      "ρ: 5269.8472\n",
      "\n",
      "Comparison with Example Output:\n",
      "Target 1: Predicted = 91987156992.0000, Example = 47665708776.4155, Difference = 44321447936.0000\n",
      "Target 2: Predicted = -4.3636, Example = 0.0159, Difference = 4.3795\n",
      "Target 3: Predicted = 430.8117, Example = 496.3562, Difference = 65.5445\n",
      "Target 4: Predicted = 0.0001, Example = 0.0001, Difference = 0.0000\n",
      "Target 5: Predicted = 0.0043, Example = 0.0000, Difference = 0.0043\n",
      "Target 6: Predicted = 0.0000, Example = -0.0000, Difference = 0.0000\n",
      "Target 7: Predicted = 5269.8472, Example = 5320.0000, Difference = 50.1528\n"
     ]
    }
   ],
   "source": [
    "# -------- Prediction Function --------\n",
    "def predict_custom_input(input_list):\n",
    "    \"\"\"\n",
    "    input_list: list of 15 feature values in order\n",
    "    \"\"\"\n",
    "    if len(input_list) != 15:\n",
    "        raise ValueError(\"Input must be a list of 15 feature values.\")\n",
    "    \n",
    "    input_scaled = scaler_X.transform([input_list])\n",
    "    prediction_scaled = model.predict(input_scaled, verbose=0)\n",
    "    prediction = scaler_y.inverse_transform(prediction_scaled)\n",
    "    \n",
    "    # Display the results\n",
    "    target_columns = [\n",
    "        \"c55\", \"e15\", \"q15\",\n",
    "        \"ϵ11\", \"μ11\", \"α11\", \"ρ\"\n",
    "    ]\n",
    "    result = dict(zip(target_columns, prediction[0]))\n",
    "    print(\"Predicted Properties:\")\n",
    "    for k, v in result.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    return prediction[0]\n",
    "\n",
    "# -------- Example of using predict_custom_input --------\n",
    "# Example Input (replace these with actual values when running)\n",
    "example_input = [\n",
    "    0.05, 44000000000.0, 11.4, 0.0, 9.82e-09, 5e-06, 0, 5700, 45300000000.0, 0, 550.0, 8.000000000000001e-11, 0.000157, 0.0, 5300\n",
    "]\n",
    "\n",
    "example_output = [\n",
    "    47665708776.4155, 0.015891104909231, 496.3562235447875, 0.0001422813709311, 9.727373332703483e-11, -4.046667731750246e-12, 5320.0\n",
    "]\n",
    "\n",
    "# Get the prediction\n",
    "predicted_output = predict_custom_input(example_input)\n",
    "\n",
    "# Compare with example_output\n",
    "print(\"\\nComparison with Example Output:\")\n",
    "for i, (predicted, example) in enumerate(zip(predicted_output, example_output)):\n",
    "    print(f\"Target {i + 1}: Predicted = {predicted:.4f}, Example = {example:.4f}, Difference = {abs(predicted - example):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
